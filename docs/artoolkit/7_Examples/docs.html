<h1 id="arapp-ios-example">ARApp - iOS Example</h1>
<p>ARApp is designed specifically for the <a href="5_iOS:ios_about">iOS</a> environment, and so differs somewhat from the design of the examples for the desktop platforms. It respects the iOS&#39;s model-view-controller design pattern. Calls to ARToolKit&#39;s C functions (part of the &quot;model&quot;) are made entirely by the ARViewController class. ARViewController is a subclass of UIViewController, and is designed to be able to be reused in your own applications. When instantiated, it opens a video window, and creates an OpenGL rendering context. When dismissed, it disposes of the OpenGL context and closes the video window.</p>
<p>As provided, the ARApp example includes a MainWindow NIB file which includes an instance of the ARViewController class. The application delegate and the ARViewController are connected in the NIB. You can easily modify this design, e.g. to load a different view controller when the application is opened. You could then instantiate ARViewController via a NIB or in code.</p>
<p>The OpenGL drawing code is contained within the ARView class and its superclass EAGLView. EAGLView extends UIView and implements either an OpenGL ES 1.1 or 2.0 rendering context. ARView extends the EAGLView class to provide important functionality for the AR environment, including compositing the OpenGL drawing together with the camera image.</p>
<p>The contents of the virtual environment are abstracted into the VirtualEnvironment class (instantiated by the view controller) and the VEObject class and its subclasses.</p>
<ul>
<li>VEObject: Root class representing behaviour of a single object in the virtual environment. Includes methods for updating the pose (position and orientation) of the object, setting a &quot;local&quot; pose for the object, testing and setting object visibility and other properties.</li>
<li>VEObjectOBJ: A subclass which represents a drawable Wavefront .OBJ model file. It registers for drawing notifications from the ARView and draws the model (using the Eden glm code).</li>
<li>VEObjectOSG: Similar to VEObjectOBJ, it allows models to be loaded and drawn using the OpenSceneGraph framework. Connects to OSG via the arOSG library. Supported file types are .osg and .ive. Models may include transform-based animation, particle effects, custom lighting, and more.</li>
<li>VEObjectMovie: Loads and draws a movie file from the local file system as a video texture, using the MovieVideo class. It allows playback of MPEG4 video (with or without audio) in the virtual environment. Recommended maximum movie size is 512 pixels or less in both the vertical and horizontal dimensions.</li>
</ul>
<p>The virtual environment connects to the OpenGL drawing and the ARToolKit tracking using NSNotifications generated by the ARView and ARMarker classes. These classes uses NSNotifications to tell the virtual environment when to update object poses with newly-processed marker data, and when to draw the updated objects.</p>
<p>The ARMarker class includes a class method to read marker definitions from a file (markers.dat) and to instantiate ARMarker instances. Full support for filtering of marker poses is included in the ARMarker class. Filtering helps remove unwanted high-frequency &quot;jittering&quot; or &quot;oscillating&quot; of marker poses when the marker pose is poorly conditioned (typically when the marker is a long distance and/or perpendicular to the camera line-of-sight). Filtering can be very easily enabled by adding a line &quot;FILTER 1&quot; to a marker definition in the markers.dat file.</p>
<p>If you have any further questions about the design, or how the app fits together, please ask on the community forum (rather than by email) so that others can benefit from the answers.</p>
<h1 id="codex-interactivus-downloadable-example">Codex Interactivus - Downloadable Example</h1>
<p>This application displays models of Leonardo da Vinci’s inventions on your device.</p>
<h2 id="download-the-application">Download the Application</h2>
<ul>
<li><a href="https://play.google.com/store/apps/details?id=com.artoolworks.CodexInteractivus">Android</a></li>
<li><a href="https://itunes.apple.com/app/codex-interactivus/id844635297">iOS</a></li>
<li><a href="http://www.artoolworks.com/dist/codexinteractivus/1.0/Codex%20Interactivus%20demo%20v1.0%20setup.exe">Windows</a></li>
<li><a href="http://www.artoolworks.com/dist/codexinteractivus/1.0/Codex%20Interactivus%20demo%20v1.0.dmg">OS X</a></li>
</ul>
<h2 id="how-do-i-use-it-">How do I use it?</h2>
<ol>
<li>Before the app can be used, you will need a physical print of the page images. You can either <a href="http://www.artoolworks.com/support/attachments/Codex%20Interactivus-r3-print.pdf">download a PDF file of the pages here</a> or use the PDF supplied with the application disk image or from the in-app menu (where applicable).</li>
<li>Once the page images are printed, point the camera at the pages.</li>
</ol>
<h2 id="platform-requirements">Platform Requirements</h2>
<h3 id="android">Android</h3>
<ul>
<li>Android v2.3 or later.</li>
<li>Your Android device must have a camera.</li>
<li>If available, a data connection (e.g. wifi or cellular data) will be used to download camera calibration data for your device.</li>
</ul>
<h3 id="ios">iOS</h3>
<ul>
<li>iOS v5.1.1 or later.</li>
</ul>
<h3 id="windows">Windows</h3>
<ul>
<li>Windows XP SP 3 or later.</li>
</ul>
<h3 id="os-x">OS X</h3>
<ul>
<li>OS X 10.6 or later.</li>
</ul>
<h1 id="nftsimple-nft-example">nftSimple - NFT Example</h1>
<h2 id="about-nftsimple">About nftSimple</h2>
<p>nftSimple is an application that demonstrates natural feature tracking (NFT) alongside very simple rendering. When an NFT surface is recognized, the familiar color cube (also as rendered in the simpleLite example) will be drawn at the origin of the dataset&#39;s coordinate system.</p>
<p>For developers who are already familiar with the code of simpleLite, it will be useful to do a side-by-side comparison of the code of nftSimple. The basic flow of program operations (grab a frame, track markers, render) is very similar, however there are significant changes in how the marker information is handled, as well as the control flow in the tracking loop.</p>
<p>The NFT data set loaded by nftSimple is specified in the file &quot;bin/Data2/markers.dat&quot;. This is a simple text file that specifies the number of markers, their type, and the path to their data. As supplied, this file points to the sample &quot;pinball&quot; dataset (an image of a pinball play surface), provided in ARToolKit/bin/DataNFT. This directory contains the required image set and feature sets. The original image file, <code>pinball.jpg</code>, is available in the directory path of <code>[ARToolKit root]/doc/Marker images/</code>.</p>
<h2 id="preparation">Preparation</h2>
<h4 id="print-out-the-jpeg-file-on-a-color-printer">Print out the jpeg file on a color printer</h4>
<pre><code>  [ARToolKit root]/doc/Marker images/pinball.jpg
</code></pre><p>The image is 1637 pixels wide and 2048 pixels high, with a resolution of 220 dots per inch (dpi). When printed at 1:1 scaling, its printed size should be 189.0 mm wide and 236.5 mm tall. This should fit comfortably onto a single sheet of A4 paper (210 mm x 297 mm) or US Letter paper (215.9 mm × 279.4 mm).</p>
<p>If instead you wish to print the image on smaller paper, the image can either be cropped top and bottom, or scaled to fit. Cropping will maintain correct distance measurements in ARToolKit, at the cost of perhaps losing some of the tracking points, whereas scaling will result in measurements returned by ARToolKit being scaled by the same amount.</p>
<h4 id="connect-your-webcam-">Connect your webcam.</h4>
<ul>
<li>Work out any webcam configuration required. Generally, if your webcam produces images greater than 800x600 resolution, it is recommended to either use the dialog box (where applicable) or set an ARToolKit video configuration environment variable to choose a resolution no greater than 800x600. A resolution of 640x480 is perfectly acceptable for NFT, and the greater frame rate achievable by using this resolution rather than a higher resolution is of more advantage than a larger frame size. See <a href="2_Configuration:config_video_capture">Configuring Video Capture in ARToolKit</a> for details on this.</li>
<li>Carefully <a href="2_Configuration:config_camera_calibration">calibrate your camera</a>) and copy the calibration data into the <code>[ARToolKit root]/bin/Data2</code> directory, overwriting the file <code>camera_para.dat</code>.</li>
</ul>
<h2 id="launch-nftsimple">Launch nftSimple</h2>
<p>Windows/Mac OS X: double click the &quot;nftSimple&quot; application in ARToolKit&#39;s <code>bin</code> directory.<br>Linux: from a terminal, cd to <code>[ARToolKit root]/bin</code> and type: <code>./nftSimple</code>.</p>
<h4 id="operation">Operation</h4>
<p>Once the application is running, point the camera at the image.</p>
<p><img src="../_media/nft_example_kpm_holding_webcam.jpg" alt="KPM example"></p>
<p>The tracking should rapidly pick up the image. Once the image is recognized, the color cube will be drawn at the origin of the page&#39;s coordinate system.</p>
<h1 id="optical-optical-see-through-example">Optical - Optical See-Through Example</h1>
<p>The applications &quot;optical&quot; and &quot;opticalStereo&quot; are example applications demonstrating <em>monocular</em> and *stereo optical see-through with a <a href="2_Configuration:config_camera_calibration">calibrated camera</a> and display, respectively.</p>
<ul>
<li>It expects to load calibrated camera parameters from the file <code>Data/camera_para.dat</code> and <a href="8_Advanced_Topics:config_optical_see-through">calibrated optical see-through parameters</a> from the file Data/optical_param.dat, so be sure that you have run the calibration programs (calib_camera and calib_optical) and moved the results of your calibration into the Data directory.</li>
<li>Print out the Hiro and Kanji markers and make sure that they are exactly 80mm on each side. <em>(Unlike video-see through where a marker of incorrect size will still be overlaid with the content, in optical see-through, the axis of the camera and the axis of the display do not usually align, and so a mis-sized marker will result in radically incorrect display)</em>.</li>
<li>Run either &quot;optical&quot; for a monocular display, or &quot;opticalStereo&quot; for a stereo display.</li>
</ul>
<p>The screen will initially be blank, as the application is in optical see-through mode. Just like <a href="8_Advanced_Topics:config_optical_see-through">calib_optical</a>, if at any point you need to see the video-image (temporarily, e.g. to see if the camera is correctly focussed or has correct brightness or contrast) press the &quot;o&quot; key. While in this mode, you can press &quot;d&quot; to see the debug (binarized image) and &quot;-&quot; and &quot;+&quot; to adjust the binarization threshold to get nice black and white borders on the pattern.</p>
<p>When the Hiro marker is held in the field of view of the camera and display, you should see the OSG &quot;axes&quot; model overlaid on top of the marker. If your calibration was accurate the two should be quite well aligned (registered). If your calibration was poor, you will observe position and orientation offsets between the cube and the marker, and the 3D objects may appear to float above or below the cards or may be distorted. The same will apply if the camera has moved relative to the display since it was calibrated, or if the display has moved on your head, or if the calibration was done by another person.</p>
<p>Below is the listing for &quot;optical&quot;:</p>
<p>&lt;pre&gt;
Usage: ./optical [options]
Options:
  --vconf &lt;video parameter for the camera&gt;
  --cpara &lt;camera parameter file for the camera&gt;
  -cpara=&lt;camera parameter file for the camera&gt;
  --width w     Use display/window width of w pixels.
  --height h    Use display/window height of h pixels.
  --refresh f   Use display refresh rate of f Hz.
  --windowed    Display in window, rather than fullscreen.
  --fullscreen  Display fullscreen, rather than in window.
  -h -help --help: show this message
&lt;/pre&gt;

</p>
<p>Below is the listing for &quot;opticalStereo&quot;:</p>
<p>&lt;pre&gt;
Usage: ./opticalStereo [options]
Options:
  --vconf &lt;video parameter for the camera&gt;
  --cpara &lt;camera parameter file for the camera&gt;
  -cpara=&lt;camera parameter file for the camera&gt;
  --width w     Use display/window width of w pixels.
  --height h    Use display/window height of h pixels.
  --refresh f   Use display refresh rate of f Hz.
  --windowed    Display in window, rather than fullscreen.
  --fullscreen  Display fullscreen, rather than in window.
  --stereo [INACTIVE|DUAL_OUTPUT|QUADBUFFERED|FRAME_SEQUENTIAL|
            SIDE_BY_SIDE|OVER_UNDER|HALF_SIDE_BY_SIDE|
            OVER_UNDER_HALF_HEIGHT|ANAGLYPH_RED_BLUE|ANAGLYPH_RED_GREEN
            ROW_INTERLACED|COLUMN_INTERLACED|CHECKERBOARD].
            Select mono or stereo mode. (Not all modes supported).
  -h -help --help: show this message
&lt;/pre&gt;

</p>
<p>If you want to change the markers, you can edit the file Data/markers.dat. If you want to change the content, edit the file Data/objects.dat.</p>
<h1 id="simplelite-the-first-example-application-to-look-at">simpleLite - The First Example Application to Look At</h1>
<p>Among the variety of example applications that are bundled with ARToolKit, simpleLight is the most straight-forward. The application is an example of traditional template marker tracking. It uses very simple marker embedded with a blank (white)  marker image. When you <a href="1_Getting_Started:about_installing">install ARToolKit</a>, you can find it with the other example applications in the <code>bin</code> directory. More examples, and explanations of their techniques can be found from the sidebar category &quot;Examples&quot;.</p>
<h3 id="what-to-expect-from-from-simplelite">What To Expect From From simpleLite</h3>
<p>After executing the application, the device&#39;s webcam is activated and scans the captured video stream for the preconfigured marker mentioned above. When the marker is recognized within the webcam&#39;s view, simpleLite tracks and superimposes a multi-colored, three dimensional, cube onto the marker&#39;s video image. The cube is tracked, aligned and affixed to the displayed video stream.</p>
<h3 id="simple-artoolkit-testing-using-simplelite">Simple ARToolKit Testing Using simpleLite</h3>
<p>Running the simpleLight example application is one of the most straight-forward ways to test that your ARToolKit SDK installation is functioning correctly. It is recommended you do this when you install a new version of ARToolKit SDK.</p>
<h2 id="executing-simplelite">Executing simpleLite</h2>
<p>Before starting the simpleLight application, print out the marker file using a high printer resolution or quality settings and using the A4 or Letter paper size setting. Also set the printer settings to scale the image to fit, scaling up the image to its maximum size on the paper, keeping a square aspect ratio (1:1). The marker file to print: <a href="https://github.com/artoolkit/artoolkit5/blob/master/doc/patterns/Hiro%20pattern.pdf">ARTOOLKIT5_HOME/doc/patterns/Hiro pattern.pdf</a>.</p>
<h3 id="windows-">Windows:</h3>
<p>simpleLite can be opened by double-clicking its icon in the <code>[ARToolKit root]\bin</code> directory or from the same directory from the command line (cmd.exe) by entering <code>simpleLite.exe</code>.</p>
<h3 id="mac-os-x-">Mac OS X:</h3>
<p>Bundled applications are generated for the examples. Open the <code>[ARToolKit root]/bin</code> directory in the Finder and double-click the <code>simpleLite.app</code> example app.</p>
<h3 id="linux-">Linux:</h3>
<p>The simpleLite application can be launched from a terminal window by entering <code>./simpleLite</code> from its directory.</p>
<h2 id="introduction-to-the-source">Introduction to the Source</h2>
<p>To demonstrate in detail how to develop an application, we will step through the source code for an existing example program: simpleLite. The source code for this program is found inside your ARToolKit installation in the directory examples/simpleLite/.</p>
<p><img src="../_media/simplelite320x240mac.png" alt="simpleLite screenshot"></p>
<p>The file we will be looking at is simpleLite.c. This program simply consists of a main routine and several graphics drawing routines.</p>
<p>The functions which correspond to the six application steps previously described are shown in Table 1. The functions corresponding to steps 2 through 5 are called within the Idle() function.</p>
<h6 id="table-1-function-calls-and-code-that-corresponds-to-the-artoolkit-applications-steps-">Table 1: Function calls and code that corresponds to the ARToolKit applications steps.</h6>
<p>| ARToolKit Step                                                          | Functions                            |
|-------------------------------------------------------------------------|--------------------------------------|
| 1. Initialize the video grabbing from the camera and load the marker(s) | setupCamera and setupMarker          |
| 2. Grab a video input frame                                             | arVideoGetImage (called in mainLoop) |
| 3. Detect the markers                                                   | arDetectMarker (called in mainLoop)  |
| 4. Calculate camera transformation                                      | arGetTransMat (called in mainLoop)   |
| 5. Draw the virtual objects                                             | Display                              |
| 6. Close the application down                                           | Quit                                 |</p>
<p>The most important functions in the program related to AR are main, setupCamera, setupMarker, mainLoop, Display, and cleanup. In the remainder of this section we will explain the key pieces of code in these functions.</p>
<p>We will not cover every piece of code in simpleLite.c, but it is well commented and you should refer to the full code listing while following the text below.</p>
<p>One final piece of explanation before we begin. In simpleLite, we use just one other library, GLUT, to handle the interaction with the operating system. We use GLUT, the OpenGL utility toolkit, to do things like open a window, and handle keypresses. However, GLUT is not required, and can be replaced with any library you like, e.g. MFC on Windows, Cocoa on Mac OS X, or QT (cross platform). It is highly recommended that you become familiar with the basics of a GLUT-based OpenGL application <em>before</em> studying the code of simpleLite.c. The most effective way to do this is to read chapter 1 of the OpenGL Programming Guide, also known as the &quot;Red Book&quot;. In particular, the hello.c sample code given in chapter 1 of the <a href="http://www.opengl.org/documentation/red_book/">Red Book</a> is the basis for the simpleLite.c code.</p>
<h2 id="main">main</h2>
<p>The main routine of simpleLite performs a number of setup tasks for the application. We will step through its code explaining the AR-specific functionality. The code in main() plus two setup functions called by main corresponds to step 1 in the table above.</p>
<p>The first piece of AR specific code is near the top of main, where we declare some variables that will be used to set up the application:</p>
<p>&lt;pre&gt;
    char <em>cparam_name = &quot;Data/camera_para.dat&quot;;
    char </em>vconf = &quot;&quot;;
    char *patt_name  = &quot;Data/patt.hiro&quot;;
&lt;/pre&gt;

</p>
<p>In this block, we define the pathname of the <a href="2_Configuration:config_camera_calibration">camera parameter file</a> the application will use, the <a href="2_Configuration:config_video_capture">video capture library configuration string</a>, and the name of the marker pattern file the application will load and try to recognize.</p>
<p>Next, we see the first AR-specific function call:</p>
<p>&lt;pre&gt;
    // ----------------------------------------------------------------------------
    // Hardware setup.
    //

    if (!setupCamera(cparam_name, vconf, gARTThreshold, &amp;gARTCparam, &amp;gARHandle, &amp;gAR3DHandle)) {
        fprintf(stderr, &quot;main(): Unable to set up AR camera.\n&quot;);
        exit(-1);
    }
&lt;/pre&gt;

</p>
<p>setupCamera loads a file containing calibration parameters for a camera, opens a connection to the camera, sets some defaults (the binarization threshold in this case) and starts grabbing frames. It records its settings into 3 variables which are passed in as parameters. In our case, we will store these parameters in global variables. setupCamera is explained more fully below.</p>
<p>The next piece of code opens up a window for us to draw into. This code uses GLUT to open the window. Later, we will install some event handlers for the window, to handle redrawing, resizing etc.</p>
<p>&lt;pre&gt;
    // ----------------------------------------------------------------------------
    // Library setup.
    //

    // Set up GL context(s) for OpenGL to draw into.
    glutInitDisplayMode(GLUT_DOUBLE | GLUT_RGBA | GLUT_DEPTH);
    if (!prefWindowed) {
        if (prefRefresh) sprintf(glutGamemode, &quot;%ix%i:%i@%i&quot;, prefWidth, prefHeight, prefDepth, prefRefresh);
        else sprintf(glutGamemode, &quot;%ix%i:%i&quot;, prefWidth, prefHeight, prefDepth);
        glutGameModeString(glutGamemode);
        glutEnterGameMode();
    } else {
        glutInitWindowSize(prefWidth, prefHeight);
        glutCreateWindow(argv[0]);
    }
&lt;/pre&gt;

</p>
<p>The code uses the value of a variable &quot;prefWindowed&quot; to decide whether to open a window, or whether to use fullscreen mode. Other variables prefWidth, prefHeight, prefDepth and prefRefresh are read to decide how many pixels wide and tall, what colour bit depth to use, and whether to change the refresh rate of the display. In simpleLite, these preferences are simply held in static variables defined near the top of main.c, but you could easily extend the example to load and save these preferences from and to a file, and allow the user to change them.</p>
<p>Next, now that we have a window from GLUT, we initialize the OpenGL part of our application. In this case, we are using the ARgsub_lite library to manage the interaction between the ARToolKit video capture and tracking, and OpenGL. At this point we also print out some debugging information in the user&#39;s console (or command line window), showing the startup parameters for the application. As well, we reset ARToolKit&#39;s timer (which can be used to keep track of frame rates).</p>
<p>&lt;pre&gt;
    // Setup ARgsub_lite library for current OpenGL context.
    if ((gArglSettings = arglSetupForCurrentContext(gARHandle)) == NULL) {
        fprintf(stderr, &quot;main(): arglSetupForCurrentContext() returned error.\n&quot;);
        Quit();
    }
    debugReportMode(gARHandle, gArglSettings);
    glEnable(GL_DEPTH_TEST);
    arUtilTimerReset();
&lt;/pre&gt;

</p>
<p>The third major part of ARToolKit initialization is to load one or more markers which the camera should track. Information about the markers has previously been recorded into marker pattern files using the mk_patt utility (called &quot;marker training&quot;), so now we can load these files. In simpleLite, we will just use one marker, the default Hiro marker. The task of loading this marker and telling ARToolKit to track it is performed by the function called <code>setupMarker()</code>.</p>
<h2 id="setupcamera-and-setupmarker">setupCamera and setupMarker</h2>
<p>Before entering a real-time tracking and drawing state, we need to initialize the ARToolKit application parameters. The key parameters for an ARToolKit application are:</p>
<ul>
<li>the patterns that will be used for the pattern template matching and the virtual objects these patterns correspond to.</li>
<li>the camera characteristics of the video camera being used.</li>
</ul>
<p>setupCamera begins by opening a connection to the video camera from which images for tracking will be acquired, using <code>arVideoOpen()</code>. The parameter vconf, passed to arVideoOpen is a string that can be used to request some video configuration other than the default. The contents of the vconf string are dependent on the video library being used. More information can be found in [Configuring video capture in ARToolKit][3] At this point we also find out from the video camera library how big the images it will supply will be, and what pixel format will be used:</p>
<p>&lt;pre&gt;
    static int setupCamera(const char <em>cparam_name, char </em>vconf, int threshold, ARParam *cparam, ARHandle <strong>arhandle, AR3DHandle </strong>ar3dhandle)
    {
        ARParam         wparam;
        int             xsize, ysize;
        int             pixFormat;

        // Open the video path.
        if (arVideoOpen(vconf) &lt; 0) {
            fprintf(stderr, &quot;setupCamera(): Unable to open connection to camera.\n&quot;);
            return (FALSE);
        }

        // Find the size of the window.
        if (arVideoGetSize(&amp;xsize, &amp;ysize) &lt; 0) return (FALSE);
        fprintf(stdout, &quot;Camera image size (x,y) = (%d,%d)\n&quot;, xsize, ysize);

        // Get the format in which the camera is returning pixels.
        pixFormat = arVideoGetPixelFormat();
        if (pixFormat &lt; 0 ) {
            fprintf(stderr, &quot;setupCamera(): Camera is using unsupported pixel format.\n&quot;);
            return (FALSE);
        }
&lt;/pre&gt;

</p>
<p>Next we deal with the structures that ARToolKit uses to hold its model of the camera&#39;s parameters. These parameters are generated by the camera calibration process. (Covered in the next chapter.) The camera parameter file is loaded with the call to arParamLoad, with the path to the file being passed in a c-string as a parameter.</p>
<p>Once the camera parameters are loaded, we adjust them to match the actual video image size being supplied by the video library, and then initialize a few necessary ARToolKit structures that depend on the camera parameters:</p>
<p>&lt;pre&gt;
        // Load the camera parameters, resize for the window and init.
        if (arParamLoad(cparam_name, 1, &amp;wparam) &lt; 0) {
            fprintf(stderr, &quot;setupCamera(): Error loading parameter file %s for camera.\n&quot;, cparam_name);
            return (FALSE);
        }
        arParamChangeSize(&amp;wparam, xsize, ysize, cparam);
        fprintf(stdout, &quot;<strong><em> Camera Parameter </em></strong>\n&quot;);
        arParamDisp(cparam);

        if ((<em>arhandle = arCreateHandle(cparam)) == NULL) {
            fprintf(stderr, &quot;setupCamera(): Error: arCreateHandle.\n&quot;);
            return (FALSE);
        }
        if (arSetPixelFormat(</em>arhandle, pixFormat) &lt; 0) {
            fprintf(stderr, &quot;setupCamera(): Error: arSetPixelFormat.\n&quot;);
            return (FALSE);
        }
&lt;/pre&gt;

</p>
<p>We complete our setupCamera by setting up some defaults related to the tracking portion of ARToolKit. These include debug mode, the labelling threshold, and the structure used to hold positions of detected patterns. Finally, we start the video library capturing frames, since we will soon be ready to process them:</p>
<p>&lt;pre&gt;
        if (arSetDebugMode(<em>arhandle, AR_DEBUG_DISABLE) &lt; 0) {
            fprintf(stderr, &quot;setupCamera(): Error: arSetDebugMode.\n&quot;);
            return (FALSE);
        }
        if (arSetLabelingThresh(</em>arhandle, threshold) &lt; 0) {
            fprintf(stderr, &quot;setupCamera(): Error: arSetLabelingThresh.\n&quot;);
            return (FALSE);
        }
        if ((*ar3dhandle = ar3DCreateHandle(cparam)) == NULL) {
            fprintf(stderr, &quot;setupCamera(): Error: ar3DCreateHandle.\n&quot;);
            return (FALSE);
        }

        if (arVideoCapStart() != 0) {
            fprintf(stderr, &quot;setupCamera(): Unable to begin camera data capture.\n&quot;);
            return (FALSE);
        }

        return (TRUE);

    }
&lt;/pre&gt;

</p>
<p>The second major part of ARToolKit setup is to load pattern files for each of the patterns we wish to detect. In simpleLite, we will only track one pattern, the basic &quot;Hiro&quot; pattern. setupMarker creates a list of patterns for ARToolKit to track, and loads the Hiro pattern into it. Loading multiple patterns can be seen in the simpleVRML example, and is covered in a later chapter of the documentation.</p>
<p>&lt;pre&gt;
    static int setupMarker(const char <em>patt_name, int </em>patt_id, ARHandle <em>arhandle, ARPattHandle **pattHandle)
    {

    if ((</em>pattHandle = arPattCreateHandle()) == NULL) {
        fprintf(stderr, &quot;setupCamera(): Error: arPattCreateHandle.\n&quot;);
        return (FALSE);
    }

    // Loading only 1 pattern in this example.
    if ((<em>patt_id = arPattLoad(</em>pattHandle, patt_name)) &lt; 0) {
        fprintf(stderr, &quot;setupMarker(): pattern load error !!\n&quot;);
        arPattDeleteHandle(<em>pattHandle);
        return (FALSE);
    }

    arPattAttach(arhandle, </em>pattHandle);

    return (TRUE);

    }
&lt;/pre&gt;

</p>
<h2 id="mainloop">mainLoop</h2>
<p>This is the routine where the bulk of the ARToolKit function calls are made and it contains code corresponding to steps 2 through 5 of the required application steps.</p>
<p>First a new video frame is requested using the function arVideoGetImage. If the function returns non-NULL, a new frame has been captured, and the return value points to the buffer containing the frame&#39;s pixel data, so we save it in a global variable.</p>
<p>&lt;pre&gt;
    // Grab a video frame.
    if ((image = arVideoGetImage()) != NULL) {
        gARTImage = image;  // Save the fetched image.
        gCallCountMarkerDetect++; // Increment ARToolKit FPS counter.
&lt;/pre&gt;

</p>
<p>Every time a new frame has been acquired, it needs to be searched for markers. This is accomplished by a call to the function arDetectMarker(), passing in the pointer to the new frame, and an ARHandle. The ARHandle holds the ARToolKit marker detection settings and also stores the results of the marker detection.</p>
<p>&lt;pre&gt;
        // Detect the markers in the video frame.
        if (arDetectMarker(gARHandle, gARTImage) &lt; 0) {
            exit(-1);
        }
&lt;/pre&gt;

</p>
<p>The results of the marker detection process can now be examined in detail, to check whether they match the IDs of the marker(s) we loaded earlier. Of course, in simpleLite, we only need to check for one marker, the Hiro marker. We use a value known as the marker confidence to make sure that we have got the Hiro marker and not a marker with a different pattern.</p>
<p>&lt;pre&gt;
        // Check through the marker_info array for highest confidence
        // visible marker matching our preferred pattern.
        k = -1;
        for (j = 0; j &lt; gARHandle-&gt;marker_num; j++) {
            if (gARHandle-&gt;markerInfo[j].id == gPatt_id) {
                if (k == -1) k = j; // First marker detected.
                else if (gARHandle-&gt;markerInfo[j].cf &gt; gARHandle-&gt;markerInfo[k].cf) k = j; // Higher confidence marker detected.
            }
        }
&lt;/pre&gt;

</p>
<p>At the end of this loop, if k has been modified, then we have found the marker containing the Hiro pattern, so the last task we ask ARToolKit to perform on the marker is to retrieve its position and orientation (its &quot;pose&quot;) relative to the camera. The pose is stored in an AR3DHandle structure, which we conveniently prepared earlier (in setupCamera)!</p>
<p>If the marker is not found, we also note that fact, because if no markers are found, we should not try to draw any 3D objects in the frame. (Drawing is discussed below).</p>
<p>&lt;pre&gt;
        if (k != -1) {
            // Get the transformation between the marker and the real camera into gPatt_trans.
            err = arGetTransMatSquare(gAR3DHandle, &amp;(gARHandle-&gt;markerInfo[k]), gPatt_width, gPatt_trans);
            gPatt_found = TRUE;
        } else {
            gPatt_found = FALSE;
        }
&lt;/pre&gt;

</p>
<p>Finally, since we have a new video frame, we request that the operating system call our Display function:</p>
<p>&lt;pre&gt;
    glutPostRedisplay();
&lt;/pre&gt;

</p>
<h2 id="display">Display</h2>
<p>We can conceive of our program running two loops in parallel; one (in mainLoop()) grabs images from the camera and looks for markers in them. The other loop displays images and 3D objects over the top of detected marker positions, or other AR-related content we might want to draw.</p>
<p>These two loops run separately, because the operating system separates drawing from other regular tasks, to work more efficiently. In simpleLite, the drawing all happens in the function named Display(). (This gets called by the operating system via GLUT).</p>
<p>In the display function, we do several steps:</p>
<ol>
<li>Clear the screen and draw the most recent frame from the camera as a video background.</li>
<li>Set up the OpenGL camera projection to match the calibrated ARToolKit camera parameters.</li>
<li>Check whether we have any active markers, and if so, position the OpenGL camera view for each one to place the coordinate system origin onto the marker.</li>
<li>Draw objects on top of any active markers (using the OpenGL camera).</li>
</ol>
<p>Step 1: Clear the screen and draw the most recent frame from the camera as a video background:</p>
<p>&lt;pre&gt;
    // Select correct buffer for this context.
    glDrawBuffer(GL_BACK);
    glClear(GL_COLOR_BUFFER_BIT | GL_DEPTH_BUFFER_BIT); // Clear the buffers for new frame.

    arglDispImage(gARTImage, &amp;gARTCparam, 1.0, gArglSettings);  // zoom = 1.0.
    gARTImage = NULL;
&lt;/pre&gt;

</p>
<p>The video image is then displayed on screen. This can either be an unwarped image, or an image warped to correct for camera distortions. Unwarping the camera&#39;s distorted image helps the virtual 3D objects appear in the correct place on the video frame.</p>
<p>Step 2: Set up the OpenGL camera projection to match the calibrated ARToolKit camera parameters.</p>
<p>&lt;pre&gt;
    // Projection transformation.
    arglCameraFrustumRH(&amp;gARTCparam, VIEW_DISTANCE_MIN, VIEW_DISTANCE_MAX, p);
    glMatrixMode(GL_PROJECTION);
    glLoadMatrixd(p);
    glMatrixMode(GL_MODELVIEW);

    // Viewing transformation.
    glLoadIdentity();
&lt;/pre&gt;

</p>
<p>The call to arglCameraFrustumRH converts the camera parameters stored in gARTCparam into an OpenGL projection matrix p, which is then loaded directly, setting the OpenGL camera projection. With this, the field-of-view, etc. of the real camera will be exactly matched in the scene.</p>
<p>Step 3: Check whether we have any active markers, and if so, position the OpenGL camera view for each one to place the coordinate system origin onto the marker.</p>
<p>&lt;pre&gt;
    if (gPatt_found) {

        // Calculate the camera position relative to the marker.
        // Replace VIEW_SCALEFACTOR with 1.0 to make one drawing unit equal to 1.0 ARToolKit units (usually millimeters).
        arglCameraViewRH(gPatt_trans, m, VIEW_SCALEFACTOR);
        glLoadMatrixd(m);
&lt;/pre&gt;

</p>
<p>arglCameraViewRH converts the marker transformation (saved in mainLoop) into an OpenGL modelview matrix. These sixteen values are the position and orientation values of the real camera, so using them to set the position of the virtual camera causes any graphical objects to be drawn to appear exactly aligned with the corresponding physical marker.</p>
<p>The virtual camera position is set using the OpenGL function glLoadMatrixd(m).</p>
<p>Step 4: Draw objects on top of any active markers (using the OpenGL camera).</p>
<p>Finally, The last part of the code is the rendering of the 3D object, in this example the OpenGL color cube. This function is simply an example (look at the code for drawCube if you are curious) and can be replaced with any drawing code. Simply, at the time the draw function is called, the OpenGL coordinate system origin is exactly in the middle of the marker, with the marker lying in the x-y plane (x to the right, y upwards) and with the z axis pointing towards the viewer.</p>
<p>If you are drawing directly onto a marker, remember not to draw in the -z part of the OpenGL coordinate system, or else your drawing will look odd, as it will be drawn &quot;behind&quot; the marker.</p>
<h2 id="cleanup">cleanup</h2>
<p>The cleanup function is called to stop ARToolKit and release resources used by it, in a clean manner:</p>
<p>&lt;pre&gt;
    static void cleanup(void) {
        arglCleanup(gArglSettings);
        arPattDetach(gARHandle);
        arPattDeleteHandle(gARPattHandle);
        arVideoCapStop();
        ar3DDeleteHandle(gAR3DHandle);
        arDeleteHandle(gARHandle);
        arVideoClose();
    }
&lt;/pre&gt;

</p>
<p>Cleanup steps are generally performed in reverse order to setup steps. NB: your application may have to perform other cleanup steps, but these are the ones required by ARToolKit.</p>
<p>  Cleanup step                                                                                !Functions to be called.</p>
<hr>
<p>  remove ARToolKit&#39;s connections to OpenGL                                                    arglCleanup
  release resources used in the marker tracking                                               arPattDetach, arPattDeleteHandle, ar3DDeleteHandle, arDeleteHandle
  stop the video capture and close down the video path to free it up for other applications   arVideoCapStop, arVideoClose</p>
<p> ../_media/ Steps in cleaning up ARToolKit.</p>
<h2 id="other-bits-of-code-in-simplelite-c">Other bits of code in simpleLite.c</h2>
<p>Most of simpleLite.c is well commented, and shouldn&#39;t be hard to follow. There are a few other areas of ARToolKit-specific code, mostly relating to debug mode. Look at the definition of Keyboard() for more.</p>
<h2 id="next-steps">Next steps</h2>
<p>That&#39;s it! With these few simple function calls, you can create a fully-functioning AR application! Of course, ARToolKit has much more functionality, and can be operated at a lower level; more of this is covered in later tutorials.</p>
<p>At this stage, have a play with the simpleLite code. When you are ready, come back and read on to the next tutorial, in which we will cover drawing on top of multiple markers, and working out the relationships between markers.</p>
